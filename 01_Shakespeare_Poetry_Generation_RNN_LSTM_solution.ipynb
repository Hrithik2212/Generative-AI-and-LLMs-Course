{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hrithik2212/Generative-AI-and-LLMs-Course-by-Bajaj-Finserv/blob/main/01_Shakespeare_Poetry_Generation_RNN_LSTM_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1c42fl7aIyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "615162c0-190f-43e8-db40-f1171f22fb81"
      },
      "source": [
        "!pip install torch==1.4.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.4.0 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.4.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kinYD3010vdt"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "\n",
        "# Check whether GPU is available and can be used\n",
        "# if CUDA is found then device is set accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"Consider changing your run-time to GPU or training will be slow.\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stneSw5L77Ln"
      },
      "source": [
        "## The data: Shakespeare's sonnets\n",
        "\n",
        "Shakespeare's sonnets can be found at the following URL featuring all of his works: http://shakespeare.mit.edu/\n",
        "\n",
        "For convenience reasons we have extracted all the plain text of the sonnets: https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt into a separate textfile and have added it to the class' repository. We will thus download it from there:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTxp46sNyKNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e5f5060-89a0-4aba-d12a-edee619e7517"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/ccc-frankfurt/Practical_ML_SS21/master/week06/sonnets.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-13 06:29:24--  https://raw.githubusercontent.com/ccc-frankfurt/Practical_ML_SS21/master/week06/sonnets.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94081 (92K) [text/plain]\n",
            "Saving to: ‘sonnets.txt’\n",
            "\n",
            "sonnets.txt         100%[===================>]  91.88K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-10-13 06:29:24 (5.59 MB/s) - ‘sonnets.txt’ saved [94081/94081]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzimDdxq8oXk"
      },
      "source": [
        "We can open the text file and print an excerpt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdSreQlxy11g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5355236-b4e1-4b8a-a593-1e9608d82e2b"
      },
      "source": [
        "# Open shakespeare text file and read the data\n",
        "with open('sonnets.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# print an excerpt of the text\n",
        "print(text[:200])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From fairest creatures we desire increase,\n",
            "That thereby beauty's rose might never die,\n",
            "But as the riper should by time decease,\n",
            "His tender heir might bear his memory:\n",
            "But thou contracted to thine own \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqofvzDN8u6Z"
      },
      "source": [
        "As we are interested in a character based neural network, we will now create a mapping from the characters to numbers so that we can do our matrix calculations with numerical data. One such way is to simply replace every character with the corresponding integer in an alphabetical sequence. If we print our excerpt, we can now see the corresponding numerical values of each character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS8C0ndK0tce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4c6f750-a2bc-4cd0-df17-9f338ecf519b"
      },
      "source": [
        "# We create two dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to integers\n",
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# Encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])\n",
        "\n",
        "# Again showing the excerpt, but this time as integers\n",
        "encoded[:200]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([44, 21, 36, 57,  7, 51, 30, 29, 21, 31,  0, 54,  7, 27, 21, 31, 30,\n",
              "       54, 22, 21, 31,  0,  7, 28, 31,  7, 35, 31,  0, 29, 21, 31,  7, 29,\n",
              "       41, 27, 21, 31, 30,  0, 31, 55, 13, 25, 37, 30, 54,  7, 54, 37, 31,\n",
              "       21, 31, 59, 47,  7, 59, 31, 30, 22, 54, 47, 16,  0,  7, 21, 36,  0,\n",
              "       31,  7, 57, 29, 53, 37, 54,  7, 41, 31, 38, 31, 21,  7, 35, 29, 31,\n",
              "       55, 13,  3, 22, 54,  7, 30,  0,  7, 54, 37, 31,  7, 21, 29, 12, 31,\n",
              "       21,  7,  0, 37, 36, 22, 52, 35,  7, 59, 47,  7, 54, 29, 57, 31,  7,\n",
              "       35, 31, 27, 31, 30,  0, 31, 55, 13, 48, 29,  0,  7, 54, 31, 41, 35,\n",
              "       31, 21,  7, 37, 31, 29, 21,  7, 57, 29, 53, 37, 54,  7, 59, 31, 30,\n",
              "       21,  7, 37, 29,  0,  7, 57, 31, 57, 36, 21, 47,  9, 13,  3, 22, 54,\n",
              "        7, 54, 37, 36, 22,  7, 27, 36, 41, 54, 21, 30, 27, 54, 31, 35,  7,\n",
              "       54, 36,  7, 54, 37, 29, 41, 31,  7, 36, 28, 41,  7])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZCdTzB39dNl"
      },
      "source": [
        "### Data loader: batching\n",
        "\n",
        "We now have our entire text file encoded as integers, which serves as our dataset. Next, we will need to define our data loader, mainly the part that is missing, the random sampling of batches. Let us define this method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWq80TVX1DNo"
      },
      "source": [
        "# Defining method to make mini-batches for training\n",
        "def get_batches(arr, batch_size, seq_length):\n",
        "    # determine the flattened batch size, i.e. sequence length times batch size\n",
        "    batch_size_total = batch_size * seq_length\n",
        "    # total number of batches we can make\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "\n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "\n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWDjvcQW92ze"
      },
      "source": [
        "### Targets/Labels\n",
        "\n",
        "We will be treating our problem as a classification task, where given an input the task is to predict the likelihood of the next character, i.e. we choose the class/character with the highest probability of a SoftMax output. Our model's output is thus a vector containing a probability for each unique character.\n",
        "\n",
        "Since we want to be able to feed our model's output back as input for the next time step, we should also give the network a one-hot encoded character as the input instead of just an integer, similar to what we have seen on our lecture's last slide.\n",
        "This way the network gets as input a one-hot vector of length corresponding to the number of total unique characters and predicts the likelihood for each character as output (for the next character in the sequence). We will thus write a function that converts our encoded characters from integers to one-hot vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l9QohlqnU-B"
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "\n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "\n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "\n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "\n",
        "    return one_hot"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GHm8hBaALrD"
      },
      "source": [
        "## A simple RNN\n",
        "\n",
        "We will start with writing a simple RNN in PyTorch. To get a better understanding of how the RNN model works, we will not be using PyTorch's convenience RNN implementation, but write the main portion by hand ourselves. We will later use the convenience functions for the much more complicated LSTMs.\n",
        "\n",
        "Note that we could in principle do the same thing in pure Numpy but the advantage of implementing the forward logic in PyTorch is that we can use the automatic differentation for our backward pass and we do not need to implement the backpropagation through time ourselves.\n",
        "\n",
        "What we will learn here is:\n",
        "1. How to write a recurrent neural network (the forward pass)\n",
        "2. How to implement custom mathematical equations in the forward pass of a PyTorch model and leverage the automatic backward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cgkv_TmovgBY"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, chars, device, hidden_sz, drop_prob=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        # creating character dictionaries\n",
        "        # we already have this code on the top, but giving it to our model\n",
        "        # will be convenient for doing predictions later\n",
        "        # i.e. doing conversions from text to integers to one-hot & vice-versa\n",
        "        self.n_chars = len(chars)\n",
        "        self.int2char = dict(enumerate(chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "        self.hidden_sz = hidden_sz\n",
        "\n",
        "        # Note that this class inherits from the torch neural network class\n",
        "        # Instead of using a pre-built function we will write the math ourselves\n",
        "        # For this reason we will first need to define \"Parameters()\", that\n",
        "        # the PyTorch graph keeps track of and can optimize. In other words,\n",
        "        # let's give our class the weights & the bias that the RNN will need.\n",
        "        self.weight_ih = Parameter(torch.Tensor(self.n_chars, self.hidden_sz))\n",
        "        self.weight_hh = Parameter(torch.Tensor(self.hidden_sz, self.hidden_sz))\n",
        "        self.bias_hh = Parameter(torch.Tensor(self.hidden_sz))\n",
        "\n",
        "        # Now that we have defined the RNN cell, let us define the output layer\n",
        "        # We will use a dropout layer to prevent overfitting and then\n",
        "        # follow with a conventional linear layer (matrix multiplication) that\n",
        "        # maps the RNN cell's output (the hidden state of the network) to the\n",
        "        # class output. Remembert that the class output corresponds to a\n",
        "        # vector of length of unique characters.\n",
        "\n",
        "        # define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        # define the final, fully-connected output layer. We can use a\n",
        "        # PyTorch nn function here (or you could add the corresponding math\n",
        "        # below and assign an additional weight & bias at the top).\n",
        "        # We can see that we can create very custom models this way\n",
        "        self.fc = nn.Linear(self.hidden_sz, self.n_chars)\n",
        "\n",
        "        # We have assigned the Parameters above, but we will need to also\n",
        "        # initialize them. Let's write a function for that and initialize\n",
        "        # our weights and bias.\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        nn.init.xavier_uniform_(self.weight_ih)\n",
        "        nn.init.xavier_uniform_(self.weight_hh)\n",
        "        nn.init.zeros_(self.bias_hh)\n",
        "\n",
        "    def forward(self, x, h_t):\n",
        "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
        "        bs, seq_sz, _ = x.size()\n",
        "        hidden_seq = []\n",
        "\n",
        "        # Given an input and an initial hidden state, calculate the next hidden\n",
        "        # state for each sequence element.\n",
        "        # We append all the hidden states to a list (similar to a batch size)\n",
        "        # so that we can concatenate them in the batch and feed them to our\n",
        "        # last linear layer all in parallel to avoid looping through the final\n",
        "        # output layer as there is no more dependence on other time steps.\n",
        "        for t in range(seq_sz):\n",
        "            x_t = x[:, t, :]\n",
        "            h_t = torch.tanh(x_t @ self.weight_ih + h_t @ self.weight_hh + self.bias_hh)\n",
        "            hidden_seq.append(h_t.unsqueeze(0))\n",
        "\n",
        "        # Do the concatenation and reshaping for convenience\n",
        "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
        "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
        "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
        "\n",
        "        # Stack up the RNN outputs using view so that we can process the last\n",
        "        # layer in parallel\n",
        "        r_output = hidden_seq.contiguous().view(-1, self.hidden_sz)\n",
        "\n",
        "        # pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "\n",
        "        # Calculate fully connected layer output that yields our class vector\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, h_t\n",
        "\n",
        "    def init_hidden(self, batch_size=1):\n",
        "        ''' Initializes hidden state '''\n",
        "        # This is a convenience function so that we can initialize a hidden\n",
        "        # state to zero when we start prediction on a sequence. Every further\n",
        "        # step will then depend on the previous hidden state.\n",
        "\n",
        "        # Create two new tensors with sizes batch_size x n_hidden,\n",
        "        # initialized to zero for hidden the RNN's hidden state.\n",
        "        weight = next(self.parameters()).data\n",
        "        h_t = weight.new(batch_size, self.hidden_sz).zero_().to(device)\n",
        "\n",
        "        return h_t\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhNyXgWuFK2T"
      },
      "source": [
        "The only thing missing is our training loop. It will look very similar to everything we have previously written, with two main differences:\n",
        "\n",
        "1. Our model is now also dependent on the hidden state and thus takes it as input and returns it as an additional output.\n",
        "2. Because we are using a recurrent neural network we will need to give our \"loss.backward()\" a \"retain_graph=True\" flag in order for it to log the history and be able to compute the backpropagation through time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSrhA4lR2EaP"
      },
      "source": [
        "# Declaring the train method\n",
        "def train(model, data, device, optimizer, criterion, epochs=10, batch_size=10,\n",
        "          seq_length=50, clip=5):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # initialize first hidden states with zeros\n",
        "        h = model.init_hidden(batch_size)\n",
        "\n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            # One-hot encode our data, make them torch tensors & cast to device\n",
        "            x = one_hot_encode(x, model.n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # get the output and hidden state from the model\n",
        "            output, h = model(inputs, h)\n",
        "\n",
        "            # calculate the loss and perform backprop\n",
        "            # because we have flattened our batch and sequence in the model to\n",
        "            # be able to speed up the connection of the last fully-connected\n",
        "            # layer we now also need to view/flatten our target here\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "            # we use an additional trick of clipping gradients to avoid\n",
        "            # exploding gradients, which is a prominent problem in RNNs, just\n",
        "            # as the opposite problem of vanishing gradients.\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "        print(\"Epoch: {}/{}:\".format(epoch + 1, epochs),\n",
        "              \"Loss: {:.4f}:\".format(loss.item()))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFKl2jWi2GeC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "515fcf4b-8c0d-41d9-99da-e1162f141cbe"
      },
      "source": [
        "# Define the model\n",
        "n_hidden=512\n",
        "model = RNN(chars, device, n_hidden).to(device)\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "seq_length = 100\n",
        "epochs = 300 # start with 50 or similar if you are debugging\n",
        "# train much longer if you want good results\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# train the model\n",
        "train(model, encoded, device, optimizer, criterion, epochs=epochs,\n",
        "      batch_size=batch_size, seq_length=seq_length)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/300: Loss: 3.2441:\n",
            "Epoch: 2/300: Loss: 3.2214:\n",
            "Epoch: 3/300: Loss: 4.5690:\n",
            "Epoch: 4/300: Loss: 3.6687:\n",
            "Epoch: 5/300: Loss: 3.1996:\n",
            "Epoch: 6/300: Loss: 3.1733:\n",
            "Epoch: 7/300: Loss: 3.1450:\n",
            "Epoch: 8/300: Loss: 3.1204:\n",
            "Epoch: 9/300: Loss: 3.1001:\n",
            "Epoch: 10/300: Loss: 3.0796:\n",
            "Epoch: 11/300: Loss: 3.0512:\n",
            "Epoch: 12/300: Loss: 3.0297:\n",
            "Epoch: 13/300: Loss: 3.0018:\n",
            "Epoch: 14/300: Loss: 2.9639:\n",
            "Epoch: 15/300: Loss: 2.9215:\n",
            "Epoch: 16/300: Loss: 2.8772:\n",
            "Epoch: 17/300: Loss: 2.8290:\n",
            "Epoch: 18/300: Loss: 2.7841:\n",
            "Epoch: 19/300: Loss: 2.7802:\n",
            "Epoch: 20/300: Loss: 2.7436:\n",
            "Epoch: 21/300: Loss: 2.6916:\n",
            "Epoch: 22/300: Loss: 2.6516:\n",
            "Epoch: 23/300: Loss: 2.6072:\n",
            "Epoch: 24/300: Loss: 2.6379:\n",
            "Epoch: 25/300: Loss: 2.5845:\n",
            "Epoch: 26/300: Loss: 2.5360:\n",
            "Epoch: 27/300: Loss: 2.5383:\n",
            "Epoch: 28/300: Loss: 2.4883:\n",
            "Epoch: 29/300: Loss: 2.4522:\n",
            "Epoch: 30/300: Loss: 2.4301:\n",
            "Epoch: 31/300: Loss: 2.4346:\n",
            "Epoch: 32/300: Loss: 2.3976:\n",
            "Epoch: 33/300: Loss: 2.3754:\n",
            "Epoch: 34/300: Loss: 2.3572:\n",
            "Epoch: 35/300: Loss: 2.3400:\n",
            "Epoch: 36/300: Loss: 2.3327:\n",
            "Epoch: 37/300: Loss: 2.3321:\n",
            "Epoch: 38/300: Loss: 2.3028:\n",
            "Epoch: 39/300: Loss: 2.2926:\n",
            "Epoch: 40/300: Loss: 2.2827:\n",
            "Epoch: 41/300: Loss: 2.2765:\n",
            "Epoch: 42/300: Loss: 2.3014:\n",
            "Epoch: 43/300: Loss: 2.2672:\n",
            "Epoch: 44/300: Loss: 2.2550:\n",
            "Epoch: 45/300: Loss: 2.2420:\n",
            "Epoch: 46/300: Loss: 2.2354:\n",
            "Epoch: 47/300: Loss: 2.2270:\n",
            "Epoch: 48/300: Loss: 2.2253:\n",
            "Epoch: 49/300: Loss: 2.2070:\n",
            "Epoch: 50/300: Loss: 2.2038:\n",
            "Epoch: 51/300: Loss: 2.1966:\n",
            "Epoch: 52/300: Loss: 2.1899:\n",
            "Epoch: 53/300: Loss: 2.1822:\n",
            "Epoch: 54/300: Loss: 2.1843:\n",
            "Epoch: 55/300: Loss: 2.1819:\n",
            "Epoch: 56/300: Loss: 2.2066:\n",
            "Epoch: 57/300: Loss: 2.1827:\n",
            "Epoch: 58/300: Loss: 2.1838:\n",
            "Epoch: 59/300: Loss: 2.1669:\n",
            "Epoch: 60/300: Loss: 2.1622:\n",
            "Epoch: 61/300: Loss: 2.1547:\n",
            "Epoch: 62/300: Loss: 2.1514:\n",
            "Epoch: 63/300: Loss: 2.1430:\n",
            "Epoch: 64/300: Loss: 2.1415:\n",
            "Epoch: 65/300: Loss: 2.1399:\n",
            "Epoch: 66/300: Loss: 2.1315:\n",
            "Epoch: 67/300: Loss: 2.1222:\n",
            "Epoch: 68/300: Loss: 2.1318:\n",
            "Epoch: 69/300: Loss: 2.1192:\n",
            "Epoch: 70/300: Loss: 2.1272:\n",
            "Epoch: 71/300: Loss: 2.1175:\n",
            "Epoch: 72/300: Loss: 2.1081:\n",
            "Epoch: 73/300: Loss: 2.1127:\n",
            "Epoch: 74/300: Loss: 2.1014:\n",
            "Epoch: 75/300: Loss: 2.1132:\n",
            "Epoch: 76/300: Loss: 2.0947:\n",
            "Epoch: 77/300: Loss: 2.0965:\n",
            "Epoch: 78/300: Loss: 2.0971:\n",
            "Epoch: 79/300: Loss: 2.0836:\n",
            "Epoch: 80/300: Loss: 2.0889:\n",
            "Epoch: 81/300: Loss: 2.1076:\n",
            "Epoch: 82/300: Loss: 2.0820:\n",
            "Epoch: 83/300: Loss: 2.0797:\n",
            "Epoch: 84/300: Loss: 2.0834:\n",
            "Epoch: 85/300: Loss: 2.0631:\n",
            "Epoch: 86/300: Loss: 2.0512:\n",
            "Epoch: 87/300: Loss: 2.0503:\n",
            "Epoch: 88/300: Loss: 2.0730:\n",
            "Epoch: 89/300: Loss: 2.0633:\n",
            "Epoch: 90/300: Loss: 2.0651:\n",
            "Epoch: 91/300: Loss: 2.0481:\n",
            "Epoch: 92/300: Loss: 2.0459:\n",
            "Epoch: 93/300: Loss: 2.0270:\n",
            "Epoch: 94/300: Loss: 2.0347:\n",
            "Epoch: 95/300: Loss: 2.0192:\n",
            "Epoch: 96/300: Loss: 2.0210:\n",
            "Epoch: 97/300: Loss: 2.0176:\n",
            "Epoch: 98/300: Loss: 2.0148:\n",
            "Epoch: 99/300: Loss: 2.0291:\n",
            "Epoch: 100/300: Loss: 2.0151:\n",
            "Epoch: 101/300: Loss: 2.0106:\n",
            "Epoch: 102/300: Loss: 2.0067:\n",
            "Epoch: 103/300: Loss: 1.9944:\n",
            "Epoch: 104/300: Loss: 1.9928:\n",
            "Epoch: 105/300: Loss: 1.9901:\n",
            "Epoch: 106/300: Loss: 1.9799:\n",
            "Epoch: 107/300: Loss: 1.9830:\n",
            "Epoch: 108/300: Loss: 1.9770:\n",
            "Epoch: 109/300: Loss: 1.9801:\n",
            "Epoch: 110/300: Loss: 1.9751:\n",
            "Epoch: 111/300: Loss: 1.9731:\n",
            "Epoch: 112/300: Loss: 1.9656:\n",
            "Epoch: 113/300: Loss: 1.9642:\n",
            "Epoch: 114/300: Loss: 1.9507:\n",
            "Epoch: 115/300: Loss: 1.9555:\n",
            "Epoch: 116/300: Loss: 1.9564:\n",
            "Epoch: 117/300: Loss: 1.9629:\n",
            "Epoch: 118/300: Loss: 1.9622:\n",
            "Epoch: 119/300: Loss: 1.9347:\n",
            "Epoch: 120/300: Loss: 1.9187:\n",
            "Epoch: 121/300: Loss: 1.9225:\n",
            "Epoch: 122/300: Loss: 1.9228:\n",
            "Epoch: 123/300: Loss: 1.9255:\n",
            "Epoch: 124/300: Loss: 1.9186:\n",
            "Epoch: 125/300: Loss: 1.9149:\n",
            "Epoch: 126/300: Loss: 1.9166:\n",
            "Epoch: 127/300: Loss: 1.9082:\n",
            "Epoch: 128/300: Loss: 1.9085:\n",
            "Epoch: 129/300: Loss: 1.8888:\n",
            "Epoch: 130/300: Loss: 1.8978:\n",
            "Epoch: 131/300: Loss: 1.8856:\n",
            "Epoch: 132/300: Loss: 1.8710:\n",
            "Epoch: 133/300: Loss: 1.8703:\n",
            "Epoch: 134/300: Loss: 1.8714:\n",
            "Epoch: 135/300: Loss: 1.8591:\n",
            "Epoch: 136/300: Loss: 1.8643:\n",
            "Epoch: 137/300: Loss: 1.8551:\n",
            "Epoch: 138/300: Loss: 1.8505:\n",
            "Epoch: 139/300: Loss: 1.8628:\n",
            "Epoch: 140/300: Loss: 1.8582:\n",
            "Epoch: 141/300: Loss: 1.8611:\n",
            "Epoch: 142/300: Loss: 1.8458:\n",
            "Epoch: 143/300: Loss: 1.8448:\n",
            "Epoch: 144/300: Loss: 1.8321:\n",
            "Epoch: 145/300: Loss: 1.8446:\n",
            "Epoch: 146/300: Loss: 1.8358:\n",
            "Epoch: 147/300: Loss: 1.8450:\n",
            "Epoch: 148/300: Loss: 1.8094:\n",
            "Epoch: 149/300: Loss: 1.8127:\n",
            "Epoch: 150/300: Loss: 1.8085:\n",
            "Epoch: 151/300: Loss: 1.8019:\n",
            "Epoch: 152/300: Loss: 1.8219:\n",
            "Epoch: 153/300: Loss: 1.8171:\n",
            "Epoch: 154/300: Loss: 1.7933:\n",
            "Epoch: 155/300: Loss: 1.7767:\n",
            "Epoch: 156/300: Loss: 1.7547:\n",
            "Epoch: 157/300: Loss: 1.7661:\n",
            "Epoch: 158/300: Loss: 1.7685:\n",
            "Epoch: 159/300: Loss: 1.7743:\n",
            "Epoch: 160/300: Loss: 1.7513:\n",
            "Epoch: 161/300: Loss: 1.7514:\n",
            "Epoch: 162/300: Loss: 1.7477:\n",
            "Epoch: 163/300: Loss: 1.7385:\n",
            "Epoch: 164/300: Loss: 1.7321:\n",
            "Epoch: 165/300: Loss: 1.7465:\n",
            "Epoch: 166/300: Loss: 1.7362:\n",
            "Epoch: 167/300: Loss: 1.7291:\n",
            "Epoch: 168/300: Loss: 1.7245:\n",
            "Epoch: 169/300: Loss: 1.7068:\n",
            "Epoch: 170/300: Loss: 1.7335:\n",
            "Epoch: 171/300: Loss: 1.7235:\n",
            "Epoch: 172/300: Loss: 1.7130:\n",
            "Epoch: 173/300: Loss: 1.7178:\n",
            "Epoch: 174/300: Loss: 1.7043:\n",
            "Epoch: 175/300: Loss: 1.7033:\n",
            "Epoch: 176/300: Loss: 1.6903:\n",
            "Epoch: 177/300: Loss: 1.7105:\n",
            "Epoch: 178/300: Loss: 1.7354:\n",
            "Epoch: 179/300: Loss: 1.7150:\n",
            "Epoch: 180/300: Loss: 1.7073:\n",
            "Epoch: 181/300: Loss: 1.6830:\n",
            "Epoch: 182/300: Loss: 1.6817:\n",
            "Epoch: 183/300: Loss: 1.6782:\n",
            "Epoch: 184/300: Loss: 1.6823:\n",
            "Epoch: 185/300: Loss: 1.6527:\n",
            "Epoch: 186/300: Loss: 1.6576:\n",
            "Epoch: 187/300: Loss: 1.6651:\n",
            "Epoch: 188/300: Loss: 1.6424:\n",
            "Epoch: 189/300: Loss: 1.6625:\n",
            "Epoch: 190/300: Loss: 1.6460:\n",
            "Epoch: 191/300: Loss: 1.6512:\n",
            "Epoch: 192/300: Loss: 1.6473:\n",
            "Epoch: 193/300: Loss: 1.6348:\n",
            "Epoch: 194/300: Loss: 1.6365:\n",
            "Epoch: 195/300: Loss: 1.6148:\n",
            "Epoch: 196/300: Loss: 1.6509:\n",
            "Epoch: 197/300: Loss: 1.6340:\n",
            "Epoch: 198/300: Loss: 1.6332:\n",
            "Epoch: 199/300: Loss: 1.6355:\n",
            "Epoch: 200/300: Loss: 1.6038:\n",
            "Epoch: 201/300: Loss: 1.6142:\n",
            "Epoch: 202/300: Loss: 1.5932:\n",
            "Epoch: 203/300: Loss: 1.5731:\n",
            "Epoch: 204/300: Loss: 1.5949:\n",
            "Epoch: 205/300: Loss: 1.5841:\n",
            "Epoch: 206/300: Loss: 1.6098:\n",
            "Epoch: 207/300: Loss: 1.5751:\n",
            "Epoch: 208/300: Loss: 1.5510:\n",
            "Epoch: 209/300: Loss: 1.5658:\n",
            "Epoch: 210/300: Loss: 1.5897:\n",
            "Epoch: 211/300: Loss: 1.5634:\n",
            "Epoch: 212/300: Loss: 1.5621:\n",
            "Epoch: 213/300: Loss: 1.5396:\n",
            "Epoch: 214/300: Loss: 1.5531:\n",
            "Epoch: 215/300: Loss: 1.5431:\n",
            "Epoch: 216/300: Loss: 1.5340:\n",
            "Epoch: 217/300: Loss: 1.5535:\n",
            "Epoch: 218/300: Loss: 1.5458:\n",
            "Epoch: 219/300: Loss: 1.5489:\n",
            "Epoch: 220/300: Loss: 1.5238:\n",
            "Epoch: 221/300: Loss: 1.5435:\n",
            "Epoch: 222/300: Loss: 1.5491:\n",
            "Epoch: 223/300: Loss: 1.5287:\n",
            "Epoch: 224/300: Loss: 1.5141:\n",
            "Epoch: 225/300: Loss: 1.5246:\n",
            "Epoch: 226/300: Loss: 1.5328:\n",
            "Epoch: 227/300: Loss: 1.5013:\n",
            "Epoch: 228/300: Loss: 1.5079:\n",
            "Epoch: 229/300: Loss: 1.5092:\n",
            "Epoch: 230/300: Loss: 1.4994:\n",
            "Epoch: 231/300: Loss: 1.5219:\n",
            "Epoch: 232/300: Loss: 1.5064:\n",
            "Epoch: 233/300: Loss: 1.5073:\n",
            "Epoch: 234/300: Loss: 1.4962:\n",
            "Epoch: 235/300: Loss: 1.4787:\n",
            "Epoch: 236/300: Loss: 1.5032:\n",
            "Epoch: 237/300: Loss: 1.4779:\n",
            "Epoch: 238/300: Loss: 1.4953:\n",
            "Epoch: 239/300: Loss: 1.4664:\n",
            "Epoch: 240/300: Loss: 1.4685:\n",
            "Epoch: 241/300: Loss: 1.4796:\n",
            "Epoch: 242/300: Loss: 1.4494:\n",
            "Epoch: 243/300: Loss: 1.4322:\n",
            "Epoch: 244/300: Loss: 1.4495:\n",
            "Epoch: 245/300: Loss: 1.4446:\n",
            "Epoch: 246/300: Loss: 1.4812:\n",
            "Epoch: 247/300: Loss: 1.4786:\n",
            "Epoch: 248/300: Loss: 1.4726:\n",
            "Epoch: 249/300: Loss: 1.4656:\n",
            "Epoch: 250/300: Loss: 1.4661:\n",
            "Epoch: 251/300: Loss: 1.4310:\n",
            "Epoch: 252/300: Loss: 1.4239:\n",
            "Epoch: 253/300: Loss: 1.4491:\n",
            "Epoch: 254/300: Loss: 1.4146:\n",
            "Epoch: 255/300: Loss: 1.4351:\n",
            "Epoch: 256/300: Loss: 1.4487:\n",
            "Epoch: 257/300: Loss: 1.4505:\n",
            "Epoch: 258/300: Loss: 1.4344:\n",
            "Epoch: 259/300: Loss: 1.4325:\n",
            "Epoch: 260/300: Loss: 1.4233:\n",
            "Epoch: 261/300: Loss: 1.4293:\n",
            "Epoch: 262/300: Loss: 1.4452:\n",
            "Epoch: 263/300: Loss: 1.4637:\n",
            "Epoch: 264/300: Loss: 1.4373:\n",
            "Epoch: 265/300: Loss: 1.4552:\n",
            "Epoch: 266/300: Loss: 1.4366:\n",
            "Epoch: 267/300: Loss: 1.4343:\n",
            "Epoch: 268/300: Loss: 1.4409:\n",
            "Epoch: 269/300: Loss: 1.4066:\n",
            "Epoch: 270/300: Loss: 1.3970:\n",
            "Epoch: 271/300: Loss: 1.3824:\n",
            "Epoch: 272/300: Loss: 1.3839:\n",
            "Epoch: 273/300: Loss: 1.3745:\n",
            "Epoch: 274/300: Loss: 1.3760:\n",
            "Epoch: 275/300: Loss: 1.3939:\n",
            "Epoch: 276/300: Loss: 1.3910:\n",
            "Epoch: 277/300: Loss: 1.3801:\n",
            "Epoch: 278/300: Loss: 1.3634:\n",
            "Epoch: 279/300: Loss: 1.3815:\n",
            "Epoch: 280/300: Loss: 1.3806:\n",
            "Epoch: 281/300: Loss: 1.4461:\n",
            "Epoch: 282/300: Loss: 1.4369:\n",
            "Epoch: 283/300: Loss: 1.4011:\n",
            "Epoch: 284/300: Loss: 1.3852:\n",
            "Epoch: 285/300: Loss: 1.4091:\n",
            "Epoch: 286/300: Loss: 1.3929:\n",
            "Epoch: 287/300: Loss: 1.3795:\n",
            "Epoch: 288/300: Loss: 1.3882:\n",
            "Epoch: 289/300: Loss: 1.3708:\n",
            "Epoch: 290/300: Loss: 1.3736:\n",
            "Epoch: 291/300: Loss: 1.3780:\n",
            "Epoch: 292/300: Loss: 1.3555:\n",
            "Epoch: 293/300: Loss: 1.3251:\n",
            "Epoch: 294/300: Loss: 1.3522:\n",
            "Epoch: 295/300: Loss: 1.3661:\n",
            "Epoch: 296/300: Loss: 1.3313:\n",
            "Epoch: 297/300: Loss: 1.3646:\n",
            "Epoch: 298/300: Loss: 1.3570:\n",
            "Epoch: 299/300: Loss: 1.3530:\n",
            "Epoch: 300/300: Loss: 1.3745:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIndlrMSGMmZ"
      },
      "source": [
        "You should observe the loss sinking consistently. In fact you can observe that the model training hasn't fully converged yet. If you feel like you want to spend the time later to see how\n",
        "well you can get your RNN to perform, try training it for longer/until convergence.\n",
        "\n",
        "Once we have trained the model it will be interesting to use it for prediction. To generate new content we would like to feed in an initial sequence or even just a single character and see what the model generates for the rest of the sequence conditioned on our input.\n",
        "\n",
        "Let us write the logic for that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A53V0aKj2IfT"
      },
      "source": [
        "def predict(model, char, device, h=None, top_k=5):\n",
        "        ''' Given a character & hidden state, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "\n",
        "        # tensor inputs\n",
        "        x = np.array([[model.char2int[char]]])\n",
        "        x = one_hot_encode(x, model.n_chars)\n",
        "        inputs = torch.from_numpy(x).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # get the output of the model\n",
        "            out, h = model(inputs, h)\n",
        "\n",
        "            # get the character probabilities\n",
        "            # move to cpu for further processing with numpy etc.\n",
        "            p = F.softmax(out, dim=1).data.cpu()\n",
        "\n",
        "            # get the top characters with highest likelihood\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "            # select the likely next character with some element of randomness\n",
        "            # for more variability\n",
        "            p = p.numpy().squeeze()\n",
        "            char = np.random.choice(top_ch, p=p/p.sum())\n",
        "\n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return model.int2char[char], h"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHAMT5xTlYY0"
      },
      "source": [
        "def sample(model, size, device, prime='A', top_k=None):\n",
        "    # method to generate new text based on a \"prime\"/initial sequence.\n",
        "    # Basically, the outer loop convenience function that calls the above\n",
        "    # defined predict method.\n",
        "    model.eval() # eval mode\n",
        "\n",
        "    # Calculate model for the initial prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    with torch.no_grad():\n",
        "        # initialize hidden with 0 in the beginning. Set our batch size to 1\n",
        "        # as we wish to generate one sequence only.\n",
        "        h = model.init_hidden(batch_size=1)\n",
        "        for ch in prime:\n",
        "            char, h = predict(model, ch, device, h=h, top_k=top_k)\n",
        "\n",
        "        # append the characters to the sequence\n",
        "        chars.append(char)\n",
        "\n",
        "        # Now pass in the previous/last character and get a new one\n",
        "        # Repeat this process for the desired length of the sequence to be\n",
        "        # generated\n",
        "        for ii in range(size):\n",
        "            char, h = predict(model, chars[-1], device, h=h, top_k=top_k)\n",
        "            chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wgYQs7FH2wo"
      },
      "source": [
        "### Generating poems\n",
        "\n",
        "We are now set to call our sample method with our trained model, a prime sequence and a desired sequence length to be generated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HClso847laed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9db2965-eaf2-48c2-d040-7aa724148568"
      },
      "source": [
        "print(sample(model, 1000, device, prime='A', top_k=5))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Asd butats thy bet thoe,\n",
            "So ceny undesthanger no myre soprin,\n",
            "Whin, buis lace sele dos,\n",
            "As ancimesenong alighal I ame, all giest this dinther mo fore tors ow thes th ald, my,\n",
            "Whit thay but fram,\n",
            "In thoug thee cenes, aye ro ffan me with migliss doving ong beeass owe, and furrtwirs thou mone ale mare sheme,\n",
            "And ghaurs so recand the haiktom blound have blrok dightre se lithe mingir spabe,\n",
            "I llin the wornd' the shill my dote.\n",
            "\n",
            "Thy piets the thall my is my,\n",
            "And the ertlet me in ghiss,\n",
            "Ay seruth seared atr y, wotds woed be to facen\n",
            "Foras andewor dange,\n",
            "Aprit line syest though watthher in out romp,ish toill thou sas bold reallon, bo, wethes love than arumy the ssade, yeime st werees thee sther milet ye rasge sid dour duey siken the mist anestringhor des,n.\n",
            "Theer touthen\n",
            "For bead wite for doe thee.\n",
            "\n",
            "Ant beamty soull tike bear,\n",
            "Which sum ir beint of moul non im now est olles maise.\n",
            "Thy hispend in gaasto twor, welakend, tourer wos.\n",
            "\n",
            "Whes love y mentt an angin sil thea trthe be fthe dsald me all el\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjkj103bICMs"
      },
      "source": [
        "We can see that our RNN typically starts out correctly and sometimes is able to generate correct words but quickly goes on to generate junk as there is no long term dependencies.\n",
        "\n",
        "We will now implement a PyTorch LSTM to see how to improve upon this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPG6v0M7IVOB"
      },
      "source": [
        "## Long Short Term Memory (LSTM)\n",
        "\n",
        "Let's take our recurrent neural network class that we have defined above and replace the simple RNN cell with one or even multiple stacked LSTM cells.\n",
        "\n",
        "If you want to go for the challenge you can try implementing this by hand similarly to the RNN cell we have defined. However, if you don't want to go through the tour-de-force exercise, you can go on ahead and use PyTorch's \"nn.LSTM()\" convenience method: https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM\n",
        "\n",
        "You can try using a stack of 2 LSTM hidden layers to simply replace the RNN cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dIQmxMEDs41"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, chars, device, n_hidden=256, n_layers=2, drop_prob=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "\n",
        "        # creating character dictionaries\n",
        "        # we already have this code on the top, but giving it to our model\n",
        "        # will be convenient for doing predictions later\n",
        "        # i.e. doing conversions from text to integers to one-hot & vice-versa\n",
        "        self.n_chars = len(chars)\n",
        "        self.int2char = dict(enumerate(chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "        # define the LSTM\n",
        "        # we no longer need to care about wieght initialization as PyTorch\n",
        "        # will handle this for us now.\n",
        "        # When defining PyTorch's nn.LSTM() set \"batch_first=True\" to assign\n",
        "        # the batch size to the first dimension (instead of the sequence) to\n",
        "        # stay consistent with our RNN implementation and re-use our code.\n",
        "        self.lstm = nn.LSTM(self.n_chars, n_hidden, n_layers,\n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "\n",
        "        # define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        # define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, self.n_chars)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network.\n",
        "            The inputs are x, and the hidden & cell state in a tuple. '''\n",
        "\n",
        "        # get the outputs and the new hidden states from the LSTM.\n",
        "        # Note that the hidden variable now is a tuple of hidden and cell state\n",
        "        # in contrast to the RNN that just had the hidden state.\n",
        "        # Because we are using the PyTorch LSTM we do not need to implement\n",
        "        # the loop anymore as the sequence will be handled internally.\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "\n",
        "        # pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "\n",
        "        # Stack up the LSTM outputs using view so that we can process the last\n",
        "        # layer in parallel\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "\n",
        "        # Calculate fully connected layer output that yields our class vector\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size=1):\n",
        "        ''' Initializes hidden state '''\n",
        "        # This is a convenience function so that we can initialize the hidden\n",
        "        # states to zero when we start prediction on a sequence. Every further\n",
        "        # step will then depend on the previous hidden states (c and h).\n",
        "\n",
        "        # Create a tuple of two new tensors with sizes\n",
        "        # n_layers x batch_size x n_hidden, initialized to zero for the\n",
        "        # LSTM hidden and cell states.\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
        "\n",
        "        return hidden"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL9kCT6TK2QT"
      },
      "source": [
        "We can use the exact same code to train our newly defined LSTM model. Let's try with the same amount of hidden units and 2 LSTM cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW70FQwT2g_h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1933eea3-c1d5-4050-b9c8-f2252cffc075"
      },
      "source": [
        "# Define the model\n",
        "n_hidden=512\n",
        "n_layers=2\n",
        "\n",
        "model = LSTM(chars, device, n_hidden, n_layers).to(device)\n",
        "\n",
        "# Declaring the hyperparameters\n",
        "batch_size = 128\n",
        "seq_length = 100\n",
        "epochs = 300 # start with 50 or similar if you are debugging\n",
        "# train much longer if you want good results\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# train the model\n",
        "train(model, encoded, device, optimizer, criterion, epochs=epochs,\n",
        "      batch_size=batch_size, seq_length=seq_length)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/300: Loss: 3.2866:\n",
            "Epoch: 2/300: Loss: 3.1616:\n",
            "Epoch: 3/300: Loss: 3.1362:\n",
            "Epoch: 4/300: Loss: 3.1175:\n",
            "Epoch: 5/300: Loss: 3.1145:\n",
            "Epoch: 6/300: Loss: 3.1093:\n",
            "Epoch: 7/300: Loss: 3.1089:\n",
            "Epoch: 8/300: Loss: 3.1052:\n",
            "Epoch: 9/300: Loss: 3.1000:\n",
            "Epoch: 10/300: Loss: 3.0913:\n",
            "Epoch: 11/300: Loss: 3.0858:\n",
            "Epoch: 12/300: Loss: 3.0638:\n",
            "Epoch: 13/300: Loss: 3.0189:\n",
            "Epoch: 14/300: Loss: 2.9618:\n",
            "Epoch: 15/300: Loss: 2.8676:\n",
            "Epoch: 16/300: Loss: 2.8028:\n",
            "Epoch: 17/300: Loss: 2.7129:\n",
            "Epoch: 18/300: Loss: 2.6524:\n",
            "Epoch: 19/300: Loss: 2.5591:\n",
            "Epoch: 20/300: Loss: 2.4862:\n",
            "Epoch: 21/300: Loss: 2.4378:\n",
            "Epoch: 22/300: Loss: 2.3989:\n",
            "Epoch: 23/300: Loss: 2.3677:\n",
            "Epoch: 24/300: Loss: 2.3410:\n",
            "Epoch: 25/300: Loss: 2.3222:\n",
            "Epoch: 26/300: Loss: 2.3025:\n",
            "Epoch: 27/300: Loss: 2.2767:\n",
            "Epoch: 28/300: Loss: 2.2488:\n",
            "Epoch: 29/300: Loss: 2.2343:\n",
            "Epoch: 30/300: Loss: 2.2277:\n",
            "Epoch: 31/300: Loss: 2.1916:\n",
            "Epoch: 32/300: Loss: 2.1635:\n",
            "Epoch: 33/300: Loss: 2.1527:\n",
            "Epoch: 34/300: Loss: 2.1217:\n",
            "Epoch: 35/300: Loss: 2.1065:\n",
            "Epoch: 36/300: Loss: 2.0856:\n",
            "Epoch: 37/300: Loss: 2.0592:\n",
            "Epoch: 38/300: Loss: 2.0577:\n",
            "Epoch: 39/300: Loss: 2.0309:\n",
            "Epoch: 40/300: Loss: 2.0159:\n",
            "Epoch: 41/300: Loss: 1.9923:\n",
            "Epoch: 42/300: Loss: 1.9792:\n",
            "Epoch: 43/300: Loss: 1.9679:\n",
            "Epoch: 44/300: Loss: 1.9567:\n",
            "Epoch: 45/300: Loss: 1.9324:\n",
            "Epoch: 46/300: Loss: 1.9200:\n",
            "Epoch: 47/300: Loss: 1.9264:\n",
            "Epoch: 48/300: Loss: 1.9083:\n",
            "Epoch: 49/300: Loss: 1.8990:\n",
            "Epoch: 50/300: Loss: 1.8794:\n",
            "Epoch: 51/300: Loss: 1.8671:\n",
            "Epoch: 52/300: Loss: 1.8570:\n",
            "Epoch: 53/300: Loss: 1.8493:\n",
            "Epoch: 54/300: Loss: 1.8443:\n",
            "Epoch: 55/300: Loss: 1.8327:\n",
            "Epoch: 56/300: Loss: 1.8201:\n",
            "Epoch: 57/300: Loss: 1.8149:\n",
            "Epoch: 58/300: Loss: 1.8065:\n",
            "Epoch: 59/300: Loss: 1.7855:\n",
            "Epoch: 60/300: Loss: 1.7789:\n",
            "Epoch: 61/300: Loss: 1.7739:\n",
            "Epoch: 62/300: Loss: 1.7607:\n",
            "Epoch: 63/300: Loss: 1.7457:\n",
            "Epoch: 64/300: Loss: 1.7482:\n",
            "Epoch: 65/300: Loss: 1.7388:\n",
            "Epoch: 66/300: Loss: 1.7347:\n",
            "Epoch: 67/300: Loss: 1.7273:\n",
            "Epoch: 68/300: Loss: 1.7084:\n",
            "Epoch: 69/300: Loss: 1.7021:\n",
            "Epoch: 70/300: Loss: 1.6965:\n",
            "Epoch: 71/300: Loss: 1.6832:\n",
            "Epoch: 72/300: Loss: 1.6878:\n",
            "Epoch: 73/300: Loss: 1.6802:\n",
            "Epoch: 74/300: Loss: 1.6687:\n",
            "Epoch: 75/300: Loss: 1.6527:\n",
            "Epoch: 76/300: Loss: 1.6486:\n",
            "Epoch: 77/300: Loss: 1.6571:\n",
            "Epoch: 78/300: Loss: 1.6325:\n",
            "Epoch: 79/300: Loss: 1.6290:\n",
            "Epoch: 80/300: Loss: 1.6262:\n",
            "Epoch: 81/300: Loss: 1.6062:\n",
            "Epoch: 82/300: Loss: 1.6029:\n",
            "Epoch: 83/300: Loss: 1.5984:\n",
            "Epoch: 84/300: Loss: 1.5933:\n",
            "Epoch: 85/300: Loss: 1.5869:\n",
            "Epoch: 86/300: Loss: 1.5683:\n",
            "Epoch: 87/300: Loss: 1.5673:\n",
            "Epoch: 88/300: Loss: 1.5636:\n",
            "Epoch: 89/300: Loss: 1.5517:\n",
            "Epoch: 90/300: Loss: 1.5474:\n",
            "Epoch: 91/300: Loss: 1.5379:\n",
            "Epoch: 92/300: Loss: 1.5387:\n",
            "Epoch: 93/300: Loss: 1.5198:\n",
            "Epoch: 94/300: Loss: 1.5204:\n",
            "Epoch: 95/300: Loss: 1.5121:\n",
            "Epoch: 96/300: Loss: 1.4946:\n",
            "Epoch: 97/300: Loss: 1.4908:\n",
            "Epoch: 98/300: Loss: 1.4766:\n",
            "Epoch: 99/300: Loss: 1.4699:\n",
            "Epoch: 100/300: Loss: 1.4580:\n",
            "Epoch: 101/300: Loss: 1.4595:\n",
            "Epoch: 102/300: Loss: 1.4484:\n",
            "Epoch: 103/300: Loss: 1.4376:\n",
            "Epoch: 104/300: Loss: 1.4299:\n",
            "Epoch: 105/300: Loss: 1.4243:\n",
            "Epoch: 106/300: Loss: 1.4043:\n",
            "Epoch: 107/300: Loss: 1.4184:\n",
            "Epoch: 108/300: Loss: 1.4151:\n",
            "Epoch: 109/300: Loss: 1.3939:\n",
            "Epoch: 110/300: Loss: 1.3735:\n",
            "Epoch: 111/300: Loss: 1.3714:\n",
            "Epoch: 112/300: Loss: 1.3532:\n",
            "Epoch: 113/300: Loss: 1.3618:\n",
            "Epoch: 114/300: Loss: 1.3420:\n",
            "Epoch: 115/300: Loss: 1.3343:\n",
            "Epoch: 116/300: Loss: 1.3241:\n",
            "Epoch: 117/300: Loss: 1.2939:\n",
            "Epoch: 118/300: Loss: 1.2936:\n",
            "Epoch: 119/300: Loss: 1.2867:\n",
            "Epoch: 120/300: Loss: 1.2739:\n",
            "Epoch: 121/300: Loss: 1.2600:\n",
            "Epoch: 122/300: Loss: 1.2612:\n",
            "Epoch: 123/300: Loss: 1.2487:\n",
            "Epoch: 124/300: Loss: 1.2498:\n",
            "Epoch: 125/300: Loss: 1.2375:\n",
            "Epoch: 126/300: Loss: 1.2203:\n",
            "Epoch: 127/300: Loss: 1.2122:\n",
            "Epoch: 128/300: Loss: 1.2002:\n",
            "Epoch: 129/300: Loss: 1.1932:\n",
            "Epoch: 130/300: Loss: 1.1820:\n",
            "Epoch: 131/300: Loss: 1.1929:\n",
            "Epoch: 132/300: Loss: 1.1649:\n",
            "Epoch: 133/300: Loss: 1.1458:\n",
            "Epoch: 134/300: Loss: 1.1368:\n",
            "Epoch: 135/300: Loss: 1.1272:\n",
            "Epoch: 136/300: Loss: 1.1093:\n",
            "Epoch: 137/300: Loss: 1.0978:\n",
            "Epoch: 138/300: Loss: 1.1029:\n",
            "Epoch: 139/300: Loss: 1.0944:\n",
            "Epoch: 140/300: Loss: 1.0772:\n",
            "Epoch: 141/300: Loss: 1.0629:\n",
            "Epoch: 142/300: Loss: 1.0455:\n",
            "Epoch: 143/300: Loss: 1.0379:\n",
            "Epoch: 144/300: Loss: 1.0117:\n",
            "Epoch: 145/300: Loss: 1.0298:\n",
            "Epoch: 146/300: Loss: 1.0198:\n",
            "Epoch: 147/300: Loss: 1.0181:\n",
            "Epoch: 148/300: Loss: 0.9891:\n",
            "Epoch: 149/300: Loss: 0.9798:\n",
            "Epoch: 150/300: Loss: 0.9494:\n",
            "Epoch: 151/300: Loss: 0.9376:\n",
            "Epoch: 152/300: Loss: 0.9318:\n",
            "Epoch: 153/300: Loss: 0.9029:\n",
            "Epoch: 154/300: Loss: 0.8963:\n",
            "Epoch: 155/300: Loss: 0.8635:\n",
            "Epoch: 156/300: Loss: 0.8817:\n",
            "Epoch: 157/300: Loss: 0.8472:\n",
            "Epoch: 158/300: Loss: 0.8422:\n",
            "Epoch: 159/300: Loss: 0.8389:\n",
            "Epoch: 160/300: Loss: 0.8355:\n",
            "Epoch: 161/300: Loss: 0.8223:\n",
            "Epoch: 162/300: Loss: 0.8172:\n",
            "Epoch: 163/300: Loss: 0.8026:\n",
            "Epoch: 164/300: Loss: 0.7906:\n",
            "Epoch: 165/300: Loss: 0.7809:\n",
            "Epoch: 166/300: Loss: 0.7787:\n",
            "Epoch: 167/300: Loss: 0.7629:\n",
            "Epoch: 168/300: Loss: 0.7456:\n",
            "Epoch: 169/300: Loss: 0.7291:\n",
            "Epoch: 170/300: Loss: 0.7089:\n",
            "Epoch: 171/300: Loss: 0.7008:\n",
            "Epoch: 172/300: Loss: 0.6891:\n",
            "Epoch: 173/300: Loss: 0.6786:\n",
            "Epoch: 174/300: Loss: 0.6742:\n",
            "Epoch: 175/300: Loss: 0.6840:\n",
            "Epoch: 176/300: Loss: 0.6608:\n",
            "Epoch: 177/300: Loss: 0.6460:\n",
            "Epoch: 178/300: Loss: 0.6528:\n",
            "Epoch: 179/300: Loss: 0.6371:\n",
            "Epoch: 180/300: Loss: 0.6441:\n",
            "Epoch: 181/300: Loss: 0.6245:\n",
            "Epoch: 182/300: Loss: 0.6242:\n",
            "Epoch: 183/300: Loss: 0.6225:\n",
            "Epoch: 184/300: Loss: 0.6030:\n",
            "Epoch: 185/300: Loss: 0.5919:\n",
            "Epoch: 186/300: Loss: 0.5871:\n",
            "Epoch: 187/300: Loss: 0.5680:\n",
            "Epoch: 188/300: Loss: 0.5717:\n",
            "Epoch: 189/300: Loss: 0.5581:\n",
            "Epoch: 190/300: Loss: 0.5474:\n",
            "Epoch: 191/300: Loss: 0.5391:\n",
            "Epoch: 192/300: Loss: 0.5340:\n",
            "Epoch: 193/300: Loss: 0.5192:\n",
            "Epoch: 194/300: Loss: 0.5215:\n",
            "Epoch: 195/300: Loss: 0.5224:\n",
            "Epoch: 196/300: Loss: 0.5088:\n",
            "Epoch: 197/300: Loss: 0.5164:\n",
            "Epoch: 198/300: Loss: 0.4909:\n",
            "Epoch: 199/300: Loss: 0.4932:\n",
            "Epoch: 200/300: Loss: 0.5009:\n",
            "Epoch: 201/300: Loss: 0.4799:\n",
            "Epoch: 202/300: Loss: 0.4916:\n",
            "Epoch: 203/300: Loss: 0.4742:\n",
            "Epoch: 204/300: Loss: 0.4674:\n",
            "Epoch: 205/300: Loss: 0.4849:\n",
            "Epoch: 206/300: Loss: 0.4656:\n",
            "Epoch: 207/300: Loss: 0.4536:\n",
            "Epoch: 208/300: Loss: 0.4669:\n",
            "Epoch: 209/300: Loss: 0.4383:\n",
            "Epoch: 210/300: Loss: 0.4188:\n",
            "Epoch: 211/300: Loss: 0.4200:\n",
            "Epoch: 212/300: Loss: 0.4102:\n",
            "Epoch: 213/300: Loss: 0.3888:\n",
            "Epoch: 214/300: Loss: 0.3777:\n",
            "Epoch: 215/300: Loss: 0.3725:\n",
            "Epoch: 216/300: Loss: 0.3594:\n",
            "Epoch: 217/300: Loss: 0.3597:\n",
            "Epoch: 218/300: Loss: 0.3465:\n",
            "Epoch: 219/300: Loss: 0.3412:\n",
            "Epoch: 220/300: Loss: 0.3358:\n",
            "Epoch: 221/300: Loss: 0.3340:\n",
            "Epoch: 222/300: Loss: 0.3252:\n",
            "Epoch: 223/300: Loss: 0.3339:\n",
            "Epoch: 224/300: Loss: 0.3285:\n",
            "Epoch: 225/300: Loss: 0.3338:\n",
            "Epoch: 226/300: Loss: 0.3216:\n",
            "Epoch: 227/300: Loss: 0.3133:\n",
            "Epoch: 228/300: Loss: 0.3163:\n",
            "Epoch: 229/300: Loss: 0.3002:\n",
            "Epoch: 230/300: Loss: 0.2934:\n",
            "Epoch: 231/300: Loss: 0.3094:\n",
            "Epoch: 232/300: Loss: 0.2998:\n",
            "Epoch: 233/300: Loss: 0.2982:\n",
            "Epoch: 234/300: Loss: 0.2928:\n",
            "Epoch: 235/300: Loss: 0.2840:\n",
            "Epoch: 236/300: Loss: 0.2970:\n",
            "Epoch: 237/300: Loss: 0.2859:\n",
            "Epoch: 238/300: Loss: 0.2852:\n",
            "Epoch: 239/300: Loss: 0.2654:\n",
            "Epoch: 240/300: Loss: 0.2582:\n",
            "Epoch: 241/300: Loss: 0.2541:\n",
            "Epoch: 242/300: Loss: 0.2554:\n",
            "Epoch: 243/300: Loss: 0.2414:\n",
            "Epoch: 244/300: Loss: 0.2487:\n",
            "Epoch: 245/300: Loss: 0.2383:\n",
            "Epoch: 246/300: Loss: 0.2398:\n",
            "Epoch: 247/300: Loss: 0.2247:\n",
            "Epoch: 248/300: Loss: 0.2263:\n",
            "Epoch: 249/300: Loss: 0.2254:\n",
            "Epoch: 250/300: Loss: 0.2338:\n",
            "Epoch: 251/300: Loss: 0.2203:\n",
            "Epoch: 252/300: Loss: 0.2274:\n",
            "Epoch: 253/300: Loss: 0.2284:\n",
            "Epoch: 254/300: Loss: 0.2209:\n",
            "Epoch: 255/300: Loss: 0.2257:\n",
            "Epoch: 256/300: Loss: 0.2094:\n",
            "Epoch: 257/300: Loss: 0.2145:\n",
            "Epoch: 258/300: Loss: 0.2162:\n",
            "Epoch: 259/300: Loss: 0.2113:\n",
            "Epoch: 260/300: Loss: 0.1993:\n",
            "Epoch: 261/300: Loss: 0.2003:\n",
            "Epoch: 262/300: Loss: 0.2023:\n",
            "Epoch: 263/300: Loss: 0.2027:\n",
            "Epoch: 264/300: Loss: 0.1892:\n",
            "Epoch: 265/300: Loss: 0.1972:\n",
            "Epoch: 266/300: Loss: 0.1836:\n",
            "Epoch: 267/300: Loss: 0.1814:\n",
            "Epoch: 268/300: Loss: 0.1853:\n",
            "Epoch: 269/300: Loss: 0.1803:\n",
            "Epoch: 270/300: Loss: 0.1772:\n",
            "Epoch: 271/300: Loss: 0.1811:\n",
            "Epoch: 272/300: Loss: 0.1760:\n",
            "Epoch: 273/300: Loss: 0.1741:\n",
            "Epoch: 274/300: Loss: 0.1696:\n",
            "Epoch: 275/300: Loss: 0.1732:\n",
            "Epoch: 276/300: Loss: 0.1636:\n",
            "Epoch: 277/300: Loss: 0.1707:\n",
            "Epoch: 278/300: Loss: 0.1719:\n",
            "Epoch: 279/300: Loss: 0.1597:\n",
            "Epoch: 280/300: Loss: 0.1649:\n",
            "Epoch: 281/300: Loss: 0.1611:\n",
            "Epoch: 282/300: Loss: 0.1609:\n",
            "Epoch: 283/300: Loss: 0.1625:\n",
            "Epoch: 284/300: Loss: 0.1644:\n",
            "Epoch: 285/300: Loss: 0.1558:\n",
            "Epoch: 286/300: Loss: 0.1558:\n",
            "Epoch: 287/300: Loss: 0.1489:\n",
            "Epoch: 288/300: Loss: 0.1599:\n",
            "Epoch: 289/300: Loss: 0.1499:\n",
            "Epoch: 290/300: Loss: 0.1563:\n",
            "Epoch: 291/300: Loss: 0.1511:\n",
            "Epoch: 292/300: Loss: 0.1400:\n",
            "Epoch: 293/300: Loss: 0.1476:\n",
            "Epoch: 294/300: Loss: 0.1431:\n",
            "Epoch: 295/300: Loss: 0.1386:\n",
            "Epoch: 296/300: Loss: 0.1321:\n",
            "Epoch: 297/300: Loss: 0.1308:\n",
            "Epoch: 298/300: Loss: 0.1324:\n",
            "Epoch: 299/300: Loss: 0.1303:\n",
            "Epoch: 300/300: Loss: 0.1328:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTznlDdJK81w"
      },
      "source": [
        "We can observe that our model is able to achieve a much lower loss than before with our simple RNN implementation. This should now also be reflected when we generate/sample new sonnets.\n",
        "\n",
        "Again, you can observe that the loss still continues to improve, even after 300 epochs. For the best results, try training the model longer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4pmGQ5J2lmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "779b0e91-c2c2-45b4-ea20-e51a509093e1"
      },
      "source": [
        "# Generating new text\n",
        "print(sample(model, 1000, device, prime='A', top_k=5))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "And and mine eyes be blove,\n",
            "To like hase write now will wor but on your be\n",
            "nor. To give mose sight, that heaven this adchistender,\n",
            "And no  trose beauteous on me on thy deeds.\n",
            "Then, could, dear that is it may se precion,\n",
            "To me dear such a tervul store me face,\n",
            "Singer that worth and to breas not fids to ment.\n",
            "\n",
            "Sum'd so thou with wither best in you with me,\n",
            "Then car my vraes tith thy dust and injury\n",
            "As have what one, of me to sheping thou art,\n",
            "Astherigh though this thine eye abind.\n",
            "Thy costantith, though I mayre have my hows,\n",
            "That the sunmorn than which im hate poes so man.\n",
            "\n",
            "For and in as your my defore costeat,\n",
            "Which his meroy live in vention might,\n",
            "That in byend'd with all that alonest,\n",
            "And my doul sunc an men uplessing sheet,\n",
            "Thiuling chatk blies which thy lease so belt\n",
            "not leaves wored will much puth impate!\n",
            "How with the thangres that with his best asey,\n",
            "Mise eyes to tive, though thut in thy mint receins,\n",
            "Give time to all his world is bedte for all,\n",
            "Nor fears to thees in to the summer s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXT-5Y-rLRKt"
      },
      "source": [
        "Arguably there is still discrepancy to the original Shakespeare texts in our just generated examples. However, if we compare this output to our RNN's output, we can see that the LSTM is able to achieve much more consistency in generating proper words as well as sometimes portions of sentences that have improved in terms of grammar. There now seems to be more overall structure."
      ]
    }
  ]
}